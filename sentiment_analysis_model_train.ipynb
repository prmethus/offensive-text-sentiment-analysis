{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb1ea5-d5de-48cd-bbae-fce9c484c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67048aba-2236-4a48-b079-160454dc87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the datasets\n",
    "\n",
    "import os, zipfile, requests\n",
    "\n",
    "datasets_zip = \"cyberbullying_dataset-mendeley_data.zip\"\n",
    "datasets_dir = \"cyberbully_datasets/\"\n",
    "datasets_url = \"https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/jf4pzyvnpj-1.zip\"\n",
    "\n",
    "with open(\"cyberbullying_dataset-mendeley_data.zip\", \"wb\") as rf:\n",
    "    rf.write(requests.get(datasets_url).content)\n",
    "\n",
    "if not os.path.exists(datasets_dir):\n",
    "    os.mkdir(datasets_dir)\n",
    "    with zipfile.ZipFile(datasets_zip) as zf:\n",
    "        zf.extractall(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fec12-7e7d-4967-baab-03fb9005b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each of the datasets and storing them in a dictionary\n",
    "\n",
    "datasets = os.listdir(\"cyberbully_datasets\")\n",
    "dataframes = {}\n",
    "for dataset in datasets:\n",
    "    dataframes[dataset.replace(\".csv\", \"\")] = pd.read_csv(datasets_dir + dataset)\n",
    "    dataframes[dataset.replace(\".csv\", \"\")] = dataframes[dataset.replace(\".csv\", \"\")].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3816a3-b518-47b0-a84c-a32fdc13c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the datasets together\n",
    "\n",
    "offensive_texts_dataset = pd.DataFrame(columns=[\"Text\", \"oh_label\"])\n",
    "\n",
    "for dataframe in dataframes:\n",
    "    offensive_texts_dataset = pd.concat([offensive_texts_dataset,dataframes[dataframe][[\"Text\", \"oh_label\"]]], axis=0)\n",
    "    \n",
    "offensive_texts_dataset = offensive_texts_dataset.rename(columns={\"oh_label\":\"Offensive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c37fec-22a5-4291-b861-85cd0bdbbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset for creating training and validation splits\n",
    "\n",
    "import random\n",
    "sentences = offensive_texts_dataset[\"Text\"].to_numpy()\n",
    "labels = offensive_texts_dataset[\"Offensive\"].to_numpy()\n",
    "\n",
    "pairs = list(zip(sentences, labels))\n",
    "sentences, labels = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961631b-2d9d-427d-ba3e-733b09d7fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation splits\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_set_size = 0.90\n",
    "\n",
    "training_shape = int(len(sentences) * train_set_size)\n",
    "validation_shape = len(sentences) - training_shape\n",
    "\n",
    "sentences_for_training = sentences[0:training_shape]\n",
    "labels_for_training = np.array(labels[0:training_shape])\n",
    "\n",
    "sentences_for_validation = sentences[training_shape:]\n",
    "labels_for_validation = np.array(labels[training_shape:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83e360-b594-4aec-9053-8639f7eced82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(sentences_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258fd77-7f4f-4c1b-9b46-40e4508b4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for tokenizin and padding words in a sentence\n",
    "\n",
    "def tokenize_and_pad(texts, tokenizer, maxlen=max_length, padding='post', truncating='post'):\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen, padding=padding, truncating=truncating)\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9670f-05a2-46cc-a572-35f839c05ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and padding the training and validation data\n",
    "\n",
    "padded_training_sequences = tokenize_and_pad(sentences_for_training, tokenizer)\n",
    "padded_validation_sequences = tokenize_and_pad(sentences_for_validation, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34747855-8c5c-46a5-9a0c-dd90b408d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with TemsprFlow's Sequential API for offensive text sentiment analysis\n",
    "\n",
    "def create_and_compile_model():\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11853d4f-3860-4754-9307-00dbefebc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_and_compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab543851-5c70-43f9-a7d3-ee66f69835d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model. Using Early Stopping to stop learning when the validation accuracy stops increasing\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=3)\n",
    "\n",
    "history = model.fit(padded_training_sequences,labels_for_training, epochs=num_epochs, batch_size=batch_size,\n",
    "                    validation_data = (padded_validation_sequences, labels_for_validation), verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c164b9-601a-43db-801d-a3ae8a7fdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "uid = str(uuid.uuid1())\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3571c9-9e0b-4cc2-9c0b-330d11cd3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the change of accuracy per epoch for both training and validation sets.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor']='orange'\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(np.arange(0,len(history.history[\"accuracy\"]),1), history.history[\"accuracy\"], c=\"black\")\n",
    "plt.plot(np.arange(0,len(history.history[\"val_accuracy\"]),1), history.history[\"val_accuracy\"], c=\"white\")\n",
    "plt.legend([\"Train accuracy [Train Dataset Size: {}]\".format(training_shape), \"Validation accuracy [Validation Dataset Size: {}]\".format(validation_shape)])\n",
    "plt.savefig(os.path.join(\"models\", \"change_of_accuracy_plot_{}.jpg\".format(uid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d97a3f-fc79-4ac6-bf94-8826d47f5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model with a unique ID, so an existing model does not get overwritten\n",
    "\n",
    "model.save(os.path.join(\"models\", \"offensive-text-sentiment-model_{}.h5\".format(uid)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
