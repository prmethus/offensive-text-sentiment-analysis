{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bb1ea5-d5de-48cd-bbae-fce9c484c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile, requests\n",
    "import pandas as pd\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67048aba-2236-4a48-b079-160454dc87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the datasets\n",
    "\n",
    "datasets_zip = \"cyberbullying_dataset-mendeley_data.zip\"\n",
    "datasets_dir = \"cyberbully_datasets/\"\n",
    "datasets_url = \"https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/jf4pzyvnpj-1.zip\"\n",
    "\n",
    "with open(\"cyberbullying_dataset-mendeley_data.zip\", \"wb\") as rf:\n",
    "    rf.write(requests.get(datasets_url).content)\n",
    "\n",
    "if not os.path.exists(datasets_dir):\n",
    "    os.mkdir(datasets_dir)\n",
    "    with zipfile.ZipFile(datasets_zip) as zf:\n",
    "        zf.extractall(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885fec12-7e7d-4967-baab-03fb9005b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for each of the datasets and storing them in a dictionary\n",
    "\n",
    "datasets = os.listdir(\"cyberbully_datasets\")\n",
    "dataframes = {}\n",
    "for dataset in datasets:\n",
    "    dataframes[dataset.replace(\".csv\", \"\")] = pd.read_csv(datasets_dir + dataset)\n",
    "    dataframes[dataset.replace(\".csv\", \"\")] = dataframes[dataset.replace(\".csv\", \"\")].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3816a3-b518-47b0-a84c-a32fdc13c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the datasets together\n",
    "\n",
    "offensive_texts_dataset = pd.DataFrame(columns=[\"Text\", \"oh_label\"])\n",
    "\n",
    "for dataframe in dataframes:\n",
    "    offensive_texts_dataset = pd.concat([offensive_texts_dataset,dataframes[dataframe][[\"Text\", \"oh_label\"]]], axis=0)\n",
    "    \n",
    "offensive_texts_dataset = offensive_texts_dataset.rename(columns={\"oh_label\":\"Offensive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c37fec-22a5-4291-b861-85cd0bdbbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset for creating training and validation splits\n",
    "\n",
    "import random\n",
    "sentences = offensive_texts_dataset[\"Text\"].to_numpy()\n",
    "labels = offensive_texts_dataset[\"Offensive\"].to_numpy()\n",
    "\n",
    "pairs = list(zip(sentences, labels))\n",
    "sentences, labels = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5961631b-2d9d-427d-ba3e-733b09d7fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation splits\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_set_size = 0.90\n",
    "\n",
    "training_shape = int(len(sentences) * train_set_size)\n",
    "validation_shape = len(sentences) - training_shape\n",
    "\n",
    "sentences_for_training = sentences[0:training_shape]\n",
    "labels_for_training = np.array(labels[0:training_shape])\n",
    "\n",
    "sentences_for_validation = sentences[training_shape:]\n",
    "labels_for_validation = np.array(labels[training_shape:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a8e6c1-839b-4a2a-b47c-5830449dc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d83e360-b594-4aec-9053-8639f7eced82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(sentences_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1258fd77-7f4f-4c1b-9b46-40e4508b4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for tokenizin and padding words in a sentence\n",
    "\n",
    "def tokenize_and_pad(texts, tokenizer, maxlen=max_length, padding='post', truncating='post'):\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen, padding=padding, truncating=truncating)\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f9670f-05a2-46cc-a572-35f839c05ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and padding the training and validation data\n",
    "\n",
    "padded_training_sequences = tokenize_and_pad(sentences_for_training, tokenizer)\n",
    "padded_validation_sequences = tokenize_and_pad(sentences_for_validation, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34747855-8c5c-46a5-9a0c-dd90b408d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with TemsprFlow's Sequential API for offensive text sentiment analysis\n",
    "\n",
    "def create_and_compile_model(hp):\n",
    "\n",
    "    embedding_dim = hp.Int(\"Embedding dim\", min_value=64, max_value=512, step=64)\n",
    "    dense_1_nodes = hp.Int(\"dense_1_nodes\", min_value=8, max_value=128, step=8)\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(max_length))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length)(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(dense_1_nodes, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.0001, 0.00001])\n",
    "    \n",
    "    model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate),\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(), tfa.metrics.F1Score(num_classes=1, threshold=0.7)]\n",
    "    \n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5b2882-1683-4111-992d-25f8a24b3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_and_compile_model(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11853d4f-3860-4754-9307-00dbefebc1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project kt_dir/otsa_/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from kt_dir/otsa_/tuner0.json\n",
      "\n",
      "Search: Running Trial #11\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "Embedding dim     |512               |320               \n",
      "dense_1_nodes     |32                |80                \n",
      "learning_rate     |0.01              |0.001             \n",
      "tuner/epochs      |5                 |5                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |1                 |1                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Epoch 1/5\n",
      "12590/12590 [==============================] - 416s 33ms/step - loss: 0.2011 - binary_accuracy: 0.9242 - f1_score: 0.5843 - val_loss: 0.1204 - val_binary_accuracy: 0.9618 - val_f1_score: 0.6491\n",
      "Epoch 2/5\n",
      "11663/12590 [==========================>...] - ETA: 30s - loss: 0.1726 - binary_accuracy: 0.9339 - f1_score: 0.6526"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(create_and_compile_model,\n",
    "                     objective=kt.Objective(\"f1_score\", direction=\"max\"),\n",
    "                     max_epochs=10,\n",
    "                     factor=10,\n",
    "                     directory='kt_dir',\n",
    "                     project_name=\"otsa_\")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', min_delta=0.0001, patience=5)]\n",
    "\n",
    "tuner.search(padded_training_sequences, labels_for_training, validation_data=(padded_validation_sequences, labels_for_validation), epochs=50, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6dbbe1-acdc-4ad2-8b5d-d46a6ebfe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab543851-5c70-43f9-a7d3-ee66f69835d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6295/6295 [==============================] - 134s 21ms/step - loss: 0.1566 - binary_accuracy: 0.9382 - f1_score: 0.6913 - val_loss: 0.1132 - val_binary_accuracy: 0.9621 - val_f1_score: 0.7246\n",
      "Epoch 2/30\n",
      "6295/6295 [==============================] - 133s 21ms/step - loss: 0.1441 - binary_accuracy: 0.9430 - f1_score: 0.7199 - val_loss: 0.1084 - val_binary_accuracy: 0.9627 - val_f1_score: 0.7013\n",
      "Epoch 3/30\n",
      "6295/6295 [==============================] - 131s 21ms/step - loss: 0.1344 - binary_accuracy: 0.9465 - f1_score: 0.7385 - val_loss: 0.1107 - val_binary_accuracy: 0.9638 - val_f1_score: 0.7346\n",
      "Epoch 4/30\n",
      "6295/6295 [==============================] - 130s 21ms/step - loss: 0.1264 - binary_accuracy: 0.9504 - f1_score: 0.7576 - val_loss: 0.1145 - val_binary_accuracy: 0.9624 - val_f1_score: 0.7284\n",
      "Epoch 5/30\n",
      "6295/6295 [==============================] - 131s 21ms/step - loss: 0.1192 - binary_accuracy: 0.9536 - f1_score: 0.7748 - val_loss: 0.1138 - val_binary_accuracy: 0.9623 - val_f1_score: 0.7440\n",
      "Epoch 6/30\n",
      "6295/6295 [==============================] - 131s 21ms/step - loss: 0.1134 - binary_accuracy: 0.9559 - f1_score: 0.7871 - val_loss: 0.1185 - val_binary_accuracy: 0.9618 - val_f1_score: 0.7637\n",
      "Epoch 7/30\n",
      "6295/6295 [==============================] - 131s 21ms/step - loss: 0.1077 - binary_accuracy: 0.9583 - f1_score: 0.8018 - val_loss: 0.1256 - val_binary_accuracy: 0.9607 - val_f1_score: 0.7379\n"
     ]
    }
   ],
   "source": [
    "# Training the model. Using Early Stopping to stop learning when the validation accuracy stops increasing\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(padded_training_sequences,labels_for_training, epochs=num_epochs, batch_size=batch_size,\n",
    "                    validation_data = (padded_validation_sequences, labels_for_validation), verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c164b9-601a-43db-801d-a3ae8a7fdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "uid = str(uuid.uuid1())\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c3571c9-9e0b-4cc2-9c0b-330d11cd3c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.facecolor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m),\u001b[38;5;241m1\u001b[39m), history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\u001b[38;5;241m1\u001b[39m), history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain accuracy [Train Dataset Size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(training_shape), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy [Validation Dataset Size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(validation_shape)])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the change of accuracy per epoch for both training and validation sets.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor']='orange'\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(np.arange(0,len(history.history[\"binary_accuracy\"]),1), history.history[\"binary_accuracy\"], c=\"black\")\n",
    "plt.plot(np.arange(0,len(history.history[\"binary_val_accuracy\"]),1), history.history[\"binaryval_accuracy\"], c=\"white\")\n",
    "plt.legend([\"Train accuracy [Train Dataset Size: {}]\".format(training_shape), \"Validation accuracy [Validation Dataset Size: {}]\".format(validation_shape)])\n",
    "plt.savefig(os.path.join(\"models\", \"change_of_accuracy_plot_{}.jpg\".format(uid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d97a3f-fc79-4ac6-bf94-8826d47f5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model with a unique ID, so an existing model does not get overwritten\n",
    "\n",
    "model.save(os.path.join(\"models\", \"offensive-text-sentiment-model_{}.h5\".format(uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f810424-171e-4d50-b99f-5aca578680ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown -P 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
